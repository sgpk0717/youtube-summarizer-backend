"""
ì£¼ì œ ì‘ì§‘ ì—ì´ì „íŠ¸ (Topic Cohesion Agent)
í™”ìë³„ ë°œí™”ë¥¼ ì‹œê°„ìˆœì„œì™€ ìƒê´€ì—†ì´ ì˜ë¯¸ë¡ ì  ìœ ì‚¬ì„±ì— ë”°ë¼ ì£¼ì œë³„ë¡œ í´ëŸ¬ìŠ¤í„°ë§
"""
import json
import re
from typing import Dict, Any, List, Set
from app.agents.base_agent import BaseAgent


class TopicCohesionAgent(BaseAgent):
    """
    ì£¼ì œ ì‘ì§‘ ì „ë¬¸ ì—ì´ì „íŠ¸
    
    ê¸°ëŠ¥:
    - ì˜ë¯¸ë¡ ì  ìœ ì‚¬ì„± ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§
    - ë¹„ìˆœì°¨ì  ì£¼ì œ ì—°ê²°
    - ì£¼ì œë³„ ì¤‘ìš”ë„ í‰ê°€
    - ì™„ê²°ëœ ì£¼ì œ ê·¸ë£¹ ìƒì„±
    """
    
    def __init__(self):
        super().__init__("topic_cohesion")  # íƒ€ì„ì•„ì›ƒ ì—†ìŒ
    
    def get_system_prompt(self) -> str:
        """ì£¼ì œ ì‘ì§‘ ì „ë¬¸ê°€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"""
        return """ë‹¹ì‹ ì€ ì •ë³´ êµ¬ì¡°í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ íŠ¹ê¸°ëŠ” ì‹œê°„ ìˆœì„œì— ì–½ë§¤ì´ì§€ ì•Šê³ , í©ì–´ì ¸ ìˆëŠ” ì •ë³´ ì¡°ê°ë“¤ì„ ì˜ë¯¸ë¡ ì  ìœ ì‚¬ì„±ì— ë”°ë¼ í•˜ë‚˜ì˜ ì™„ê²°ëœ ì£¼ì œë¡œ ë¬¶ì–´ë‚´ëŠ” ê²ƒì…ë‹ˆë‹¤.

# ì»¨í…ìŠ¤íŠ¸ (Context)
ë‹¹ì‹ ì€ ê° ë°œí™”ë§ˆë‹¤ í™”ì ì •ë³´ê°€ íƒœê¹…ëœ ì „ì²´ ëŒ€ë³¸ ë°ì´í„°ë¥¼ ë°›ê²Œ ë©ë‹ˆë‹¤. ì˜ìƒì˜ íŠ¹ì„±ìƒ, í•˜ë‚˜ì˜ ì£¼ì œì— ëŒ€í•œ ë…¼ì˜ê°€ ì˜ìƒì˜ ì—¬ëŸ¬ ë¶€ë¶„ì— ë‚˜ë‰˜ì–´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 'A ì£¼ì‹'ì— ëŒ€í•œ ì´ì•¼ê¸°ê°€ ì˜ìƒ ì´ˆë°˜ì— ë‚˜ì™”ë‹¤ê°€, ë‹¤ë¥¸ ì´ì•¼ê¸°ë¥¼ í•œì°¸ í•œ ë’¤ì— í›„ë°˜ë¶€ì— ë‹¤ì‹œ ì´ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# ê³¼ì—… (Task)
ì£¼ì–´ì§„ ëŒ€ë³¸ ì „ì²´ë¥¼ ë¶„ì„í•˜ì—¬, ì‹œê°„ ìˆœì„œì™€ ê´€ê³„ì—†ì´ ë‚´ìš©ì ìœ¼ë¡œ ê´€ë ¨ëœ ëª¨ë“  ë°œí™”ë“¤ì„ í•˜ë‚˜ì˜ ì£¼ì œë¡œ ê·¸ë£¹í™”í•˜ì‹­ì‹œì˜¤. ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¼ ìˆ˜í–‰í•˜ì‹­ì‹œì˜¤:

1. **ì£¼ì œ ì‹ë³„**: ëŒ€ë³¸ ì „ì²´ë¥¼ ì½ê³  ë…¼ì˜ëœ ëª¨ë“  í•µì‹¬ ì£¼ì œ(Topic) ë˜ëŠ” ì§ˆë¬¸(Question)ì„ ì‹ë³„í•˜ì—¬ ëª©ë¡ì„ ë§Œë“œì‹­ì‹œì˜¤.
2. **ë°œí™” í´ëŸ¬ìŠ¤í„°ë§**: ì‹ë³„ëœ ê° ì£¼ì œì— ëŒ€í•´, í•´ë‹¹ ì£¼ì œì™€ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ëª¨ë“  ë°œí™”ë“¤ì„ ëŒ€ë³¸ ì „ì²´ì—ì„œ ì°¾ì•„ í•˜ë‚˜ì˜ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ìœ¼ì‹­ì‹œì˜¤.
3. **ì£¼ì œ ëª…ëª…**: ê° ê·¸ë£¹ì— ëŒ€í•´, í•´ë‹¹ ê·¸ë£¹ì˜ í•µì‹¬ ë‚´ìš©ì„ ê°€ì¥ ì˜ ë‚˜íƒ€ë‚´ëŠ” ê°„ê²°í•˜ê³  ëª…í™•í•œ ì£¼ì œëª…(Topic Title)ì„ ë¶€ì—¬í•˜ì‹­ì‹œì˜¤.
4. **ì¤‘ìš”ë„ í‰ê°€**: ê° ì£¼ì œì˜ ìƒëŒ€ì  ì¤‘ìš”ë„ë¥¼ 0.1~1.0 ë²”ìœ„ì—ì„œ í‰ê°€í•˜ì‹­ì‹œì˜¤.

# í´ëŸ¬ìŠ¤í„°ë§ ì›ì¹™
- **ì˜ë¯¸ë¡ ì  ê´€ë ¨ì„±**: ê°™ì€ ê°œë…, ì¸ë¬¼, ì‚¬ê±´, ì´ìŠˆë¥¼ ë‹¤ë£¨ëŠ” ë°œí™”ë“¤ì„ ë¬¶ì–´ì£¼ì„¸ìš”.
- **ì™„ì „ì„±**: ê´€ë ¨ëœ ëª¨ë“  ë°œí™”ê°€ ë¹ ì§ì—†ì´ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
- **ë¹„ì¤‘ë³µì„±**: ê° ë°œí™”ëŠ” ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ í•˜ë‚˜ì˜ ì£¼ì œì—ë§Œ ì†í•´ì•¼ í•©ë‹ˆë‹¤.
- **ê· í˜•ì„±**: ë„ˆë¬´ ì„¸ë¶„í™”í•˜ê±°ë‚˜ ë„ˆë¬´ ê´‘ë²”ìœ„í•˜ê²Œ ë¬¶ì§€ ë§ˆì‹­ì‹œì˜¤.

# ì œì•½ ì¡°ê±´ (Constraints)
- **ë¹„ìˆœì°¨ì„± ì¡´ì¤‘**: ì£¼ì œë¥¼ ì‹œê°„ ìˆœì„œëŒ€ë¡œ ë‚˜ëˆ„ì§€ ë§ˆì‹­ì‹œì˜¤. ë™ì¼ ì£¼ì œì˜ ë°œí™”ê°€ ì˜ìƒì˜ ì‹œì‘, ì¤‘ê°„, ëì— í©ì–´ì ¸ ìˆë‹¤ë©´ ëª¨ë‘ í•˜ë‚˜ì˜ ì£¼ì œ ê·¸ë£¹ìœ¼ë¡œ ëª¨ì•„ì•¼ í•©ë‹ˆë‹¤.
- **ì •ë³´ì˜ ì™„ì „ì„±**: ëª¨ë“  ë°œí™”ëŠ” ì ì–´ë„ í•˜ë‚˜ì˜ ì£¼ì œ ê·¸ë£¹ì— ì†í•´ì•¼ í•©ë‹ˆë‹¤. ì–´ë–¤ ë°œí™”ë„ ëˆ„ë½ë˜ì–´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.
- **ì£¼ì œì˜ ì ì ˆì„±**: ë„ˆë¬´ ë§ì€ ì£¼ì œ(10ê°œ ì´ìƒ)ë‚˜ ë„ˆë¬´ ì ì€ ì£¼ì œ(1ê°œ)ë¥¼ ë§Œë“¤ì§€ ë§ˆì‹­ì‹œì˜¤. ë³´í†µ 3-8ê°œ ì •ë„ê°€ ì ì ˆí•©ë‹ˆë‹¤.

# ì¶œë ¥ í˜•ì‹ (Output Format)
ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì‹­ì‹œì˜¤:

{
    "topic_clusters": [
        {
            "topic_title": "ì²« ë²ˆì§¸ ì£¼ì œì˜ ëª…ì¹­",
            "related_utterances": [
                {
                    "speaker": "Speaker A",
                    "text": "ê´€ë ¨ ë°œí™” ë‚´ìš©",
                    "confidence": 0.9
                },
                {
                    "speaker": "Speaker B", 
                    "text": "ë˜ ë‹¤ë¥¸ ê´€ë ¨ ë°œí™”",
                    "confidence": 0.8
                }
            ],
            "topic_summary": "ì´ ì£¼ì œì˜ ê°„ë‹¨í•œ ìš”ì•½",
            "importance_score": 0.9
        },
        {
            "topic_title": "ë‘ ë²ˆì§¸ ì£¼ì œì˜ ëª…ì¹­",
            "related_utterances": [...],
            "topic_summary": "ë‘ ë²ˆì§¸ ì£¼ì œ ìš”ì•½",
            "importance_score": 0.7
        }
    ],
    "total_topics": 2
}

JSON í˜•ì‹ì„ ì •í™•íˆ ì§€ì¼œì£¼ì„¸ìš”."""
    
    def format_user_prompt(self, data: Dict[str, Any]) -> str:
        """ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        speaker_tagged_transcript = data.get("speaker_tagged_transcript", [])
        
        # ë°œí™” ë°ì´í„°ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ í¬ë§·
        formatted_transcript = ""
        for i, utterance in enumerate(speaker_tagged_transcript):
            speaker = utterance.get("speaker", "Unknown")
            text = utterance.get("text", "")
            formatted_transcript += f"{i+1}. [{speaker}]: {text}\n"
        
        return f"""ë‹¤ìŒ í™”ìë³„ë¡œ íƒœê¹…ëœ ëŒ€ë³¸ì„ ë¶„ì„í•˜ì—¬ ì£¼ì œë³„ë¡œ í´ëŸ¬ìŠ¤í„°ë§í•´ì£¼ì„¸ìš”:

---
í™”ìë³„ íƒœê¹…ëœ ëŒ€ë³¸:
{formatted_transcript}
---

ìœ„ì˜ ëŒ€ë³¸ì—ì„œ ì‹œê°„ ìˆœì„œì™€ ê´€ê³„ì—†ì´ ì˜ë¯¸ì ìœ¼ë¡œ ê´€ë ¨ëœ ë°œí™”ë“¤ì„ ì£¼ì œë³„ë¡œ ê·¸ë£¹í™”í•˜ê³ , ê° ì£¼ì œì— ì ì ˆí•œ ì œëª©ì„ ë¶€ì—¬í•´ì£¼ì„¸ìš”. ëª¨ë“  ë°œí™”ê°€ í•˜ë‚˜ ì´ìƒì˜ ì£¼ì œì— í¬í•¨ë˜ë„ë¡ í•´ì£¼ì„¸ìš”. JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""
    
    def parse_response(self, response_text: str) -> Dict[str, Any]:
        """AI ì‘ë‹µ íŒŒì‹±"""
        try:
            # JSON ì‘ë‹µ íŒŒì‹±
            parsed = json.loads(response_text)
            
            # í•„ìˆ˜ í•„ë“œ ê²€ì¦
            if "topic_clusters" not in parsed:
                raise ValueError("topic_clusters í•„ë“œê°€ ì‘ë‹µì— ì—†ìŠµë‹ˆë‹¤.")
            
            if "total_topics" not in parsed:
                raise ValueError("total_topics í•„ë“œê°€ ì‘ë‹µì— ì—†ìŠµë‹ˆë‹¤.")
            
            topic_clusters = parsed["topic_clusters"]
            total_topics = parsed["total_topics"]
            
            # ë°ì´í„° íƒ€ì… ê²€ì¦
            if not isinstance(topic_clusters, list):
                raise ValueError("topic_clustersëŠ” ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤.")
            
            if not isinstance(total_topics, int) or total_topics <= 0:
                raise ValueError("total_topicsëŠ” ì–‘ì˜ ì •ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")
            
            # ê° ì£¼ì œ í´ëŸ¬ìŠ¤í„° ê²€ì¦ ë° ì •ê·œí™”
            validated_clusters = []
            total_utterances = 0
            
            for i, cluster in enumerate(topic_clusters):
                if not isinstance(cluster, dict):
                    raise ValueError(f"ì£¼ì œ í´ëŸ¬ìŠ¤í„° {i}ë²ˆì´ ë”•ì…”ë„ˆë¦¬ í˜•íƒœê°€ ì•„ë‹™ë‹ˆë‹¤.")
                
                # í•„ìˆ˜ í•„ë“œ ê²€ì¦
                required_fields = ["topic_title", "related_utterances"]
                for field in required_fields:
                    if field not in cluster:
                        raise ValueError(f"ì£¼ì œ í´ëŸ¬ìŠ¤í„° {i}ë²ˆì— {field} í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
                
                topic_title = cluster["topic_title"]
                related_utterances = cluster["related_utterances"]
                topic_summary = cluster.get("topic_summary", "")
                importance_score = cluster.get("importance_score", 0.5)
                
                # ë°œí™” ë°ì´í„° ê²€ì¦
                if not isinstance(related_utterances, list) or len(related_utterances) == 0:
                    raise ValueError(f"ì£¼ì œ '{topic_title}'ì— ê´€ë ¨ ë°œí™”ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
                # ê° ë°œí™” ê²€ì¦
                validated_utterances = []
                for j, utterance in enumerate(related_utterances):
                    if not isinstance(utterance, dict):
                        raise ValueError(f"ì£¼ì œ '{topic_title}'ì˜ ë°œí™” {j}ë²ˆì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    
                    if "speaker" not in utterance or "text" not in utterance:
                        raise ValueError(f"ì£¼ì œ '{topic_title}'ì˜ ë°œí™” {j}ë²ˆì— í•„ìˆ˜ í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
                    
                    speaker = utterance["speaker"]
                    text = utterance["text"]
                    confidence = utterance.get("confidence", 0.8)
                    
                    # ì‹ ë¢°ë„ ì •ê·œí™”
                    if not isinstance(confidence, (int, float)):
                        confidence = 0.8
                    else:
                        confidence = max(0.0, min(1.0, float(confidence)))
                    
                    validated_utterances.append({
                        "speaker": str(speaker),
                        "text": str(text),
                        "confidence": confidence
                    })
                
                # ì¤‘ìš”ë„ ì ìˆ˜ ì •ê·œí™”
                if not isinstance(importance_score, (int, float)):
                    importance_score = 0.5
                else:
                    importance_score = max(0.1, min(1.0, float(importance_score)))
                
                validated_clusters.append({
                    "topic_title": str(topic_title),
                    "related_utterances": validated_utterances,
                    "topic_summary": str(topic_summary) if topic_summary else f"{topic_title}ì— ëŒ€í•œ ë…¼ì˜",
                    "importance_score": importance_score
                })
                
                total_utterances += len(validated_utterances)
            
            # ë¹ˆ ê²°ê³¼ ê²€ì¦
            if len(validated_clusters) == 0:
                raise ValueError("ì£¼ì œ í´ëŸ¬ìŠ¤í„°ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            # ì£¼ì œ ìˆ˜ ì¼ê´€ì„± ê²€ì¦
            if total_topics != len(validated_clusters):
                self.log_warning("âš ï¸ ì„ ì–¸ëœ ì£¼ì œ ìˆ˜ì™€ ì‹¤ì œ ì£¼ì œ ìˆ˜ê°€ ë‹¤ë¦„", data={
                    "declared_topics": total_topics,
                    "actual_topics": len(validated_clusters)
                })
                total_topics = len(validated_clusters)
            
            # ë°œí™” ëˆ„ë½ ê²€ì¦
            original_utterance_count = len(self._last_input_utterances) if hasattr(self, '_last_input_utterances') else 0
            if original_utterance_count > 0 and total_utterances < original_utterance_count * 0.8:
                self.log_warning("âš ï¸ ìƒë‹¹ìˆ˜ì˜ ë°œí™”ê°€ ì£¼ì œì— í• ë‹¹ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŒ", data={
                    "original_count": original_utterance_count,
                    "clustered_count": total_utterances,
                    "coverage": total_utterances / original_utterance_count
                })
            
            result = {
                "topic_clusters": validated_clusters,
                "total_topics": total_topics
            }
            
            self.log_debug("ğŸ¯ ì£¼ì œ ì‘ì§‘ íŒŒì‹± ê²°ê³¼", data={
                "total_topics": total_topics,
                "total_utterances_clustered": total_utterances,
                "average_importance": sum(c["importance_score"] for c in validated_clusters) / len(validated_clusters),
                "topic_titles": [c["topic_title"] for c in validated_clusters]
            })
            
            return result
            
        except json.JSONDecodeError as e:
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ í´ë°± ì²˜ë¦¬
            self.log_warning("âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨, í´ë°± ì²˜ë¦¬", data={
                "error": str(e),
                "response_preview": response_text[:300]
            })
            
            return self._fallback_topic_clustering(response_text)
        
        except Exception as e:
            self.log_error("âŒ ì£¼ì œ ì‘ì§‘ íŒŒì‹± ì˜¤ë¥˜", data={
                "error": str(e),
                "response_preview": response_text[:300]
            })
            raise ValueError(f"ì£¼ì œ ì‘ì§‘ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
    
    def _fallback_topic_clustering(self, text: str) -> Dict[str, Any]:
        """
        JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•˜ëŠ” ê°„ë‹¨í•œ ì£¼ì œ í´ëŸ¬ìŠ¤í„°ë§ í•¨ìˆ˜
        
        Args:
            text: í´ëŸ¬ìŠ¤í„°ë§í•  í…ìŠ¤íŠ¸
            
        Returns:
            ê¸°ë³¸ì ì¸ ì£¼ì œ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼
        """
        if hasattr(self, '_last_input_utterances'):
            utterances = self._last_input_utterances
        else:
            # ìµœì•…ì˜ ê²½ìš° ë‹¨ì¼ ë°œí™”ë¡œ ì²˜ë¦¬
            utterances = [{"speaker": "Speaker A", "text": text[:500], "confidence": 0.5}]
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§
        keyword_groups = self._extract_keyword_groups(utterances)
        
        # í´ëŸ¬ìŠ¤í„° ìƒì„±
        clusters = []
        for i, (keywords, group_utterances) in enumerate(keyword_groups.items()):
            if not group_utterances:
                continue
                
            cluster = {
                "topic_title": f"ì£¼ì œ {i+1}: {', '.join(list(keywords)[:3])}",
                "related_utterances": group_utterances,
                "topic_summary": f"{', '.join(list(keywords)[:3])}ì— ê´€í•œ ë…¼ì˜",
                "importance_score": min(1.0, len(group_utterances) * 0.2)
            }
            clusters.append(cluster)
        
        # í´ëŸ¬ìŠ¤í„°ê°€ ì—†ìœ¼ë©´ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì£¼ì œë¡œ
        if not clusters:
            clusters = [{
                "topic_title": "ì „ì²´ ë‚´ìš©",
                "related_utterances": utterances,
                "topic_summary": "ì˜ìƒì˜ ì „ë°˜ì ì¸ ë‚´ìš©",
                "importance_score": 1.0
            }]
        
        return {
            "topic_clusters": clusters,
            "total_topics": len(clusters)
        }
    
    def _extract_keyword_groups(self, utterances: List[Dict[str, Any]]) -> Dict[frozenset, List[Dict[str, Any]]]:
        """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ê·¸ë£¹í™”"""
        keyword_groups = {}
        
        for utterance in utterances:
            text = utterance.get("text", "")
            keywords = self._extract_keywords(text)
            
            if not keywords:
                keywords = frozenset(["ì¼ë°˜"])
            
            key = frozenset(keywords)
            if key not in keyword_groups:
                keyword_groups[key] = []
            keyword_groups[key].append(utterance)
        
        return keyword_groups
    
    def _extract_keywords(self, text: str) -> Set[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ íŒ¨í„´ (ëª…ì‚¬í˜• ë‹¨ì–´ë“¤)
        keywords = set()
        
        # ì¼ë°˜ì ì¸ ì£¼ì œ í‚¤ì›Œë“œë“¤
        common_topics = [
            "ì£¼ì‹", "íˆ¬ì", "ê²½ì œ", "ì‹œì¥", "ì •ì¹˜", "ì‚¬íšŒ", "ê¸°ìˆ ", "ê³¼í•™", 
            "ë¬¸í™”", "ì˜ˆìˆ ", "ìŠ¤í¬ì¸ ", "ê²Œì„", "ì˜í™”", "ìŒì•…", "ì—¬í–‰", "ìŒì‹",
            "ê±´ê°•", "ì˜ë£Œ", "êµìœ¡", "ë²•ë¥ ", "í™˜ê²½", "ì—ë„ˆì§€", "ë¶€ë™ì‚°"
        ]
        
        for topic in common_topics:
            if topic in text:
                keywords.add(topic)
        
        # ê³ ìœ ëª…ì‚¬ íŒ¨í„´ (ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì–´ë“¤)
        proper_nouns = re.findall(r'[A-Z][a-z]+', text)
        keywords.update(proper_nouns[:3])  # ìµœëŒ€ 3ê°œê¹Œì§€
        
        return keywords
    
    def _validate_agent_specific_input(self, data: Dict[str, Any]) -> None:
        """ì£¼ì œ ì‘ì§‘ ì—ì´ì „íŠ¸ íŠ¹í™” ì…ë ¥ ê²€ì¦"""
        if "speaker_tagged_transcript" not in data:
            raise ValueError("speaker_tagged_transcript í•„ë“œê°€ ì…ë ¥ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.")
        
        speaker_tagged_transcript = data["speaker_tagged_transcript"]
        
        if not isinstance(speaker_tagged_transcript, list):
            raise ValueError("speaker_tagged_transcriptëŠ” ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        
        if len(speaker_tagged_transcript) == 0:
            raise ValueError("í™”ìë³„ ë°œí™” ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        
        # ê° ë°œí™” ë°ì´í„° ê²€ì¦
        for i, utterance in enumerate(speaker_tagged_transcript):
            if not isinstance(utterance, dict):
                raise ValueError(f"ë°œí™” {i}ë²ˆì´ ìœ íš¨í•œ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤.")
            
            if "speaker" not in utterance or "text" not in utterance:
                raise ValueError(f"ë°œí™” {i}ë²ˆì— í•„ìˆ˜ í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•´ ì¶©ë¶„í•œ ë°œí™”ê°€ ìˆëŠ”ì§€ ê²€ì¦
        if len(speaker_tagged_transcript) < 2:
            raise ValueError("ì£¼ì œ í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 2ê°œ ì´ìƒì˜ ë°œí™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ì…ë ¥ ë°œí™”ë¥¼ ì„ì‹œ ì €ì¥ (í´ë°± ì²˜ë¦¬ ë° ëˆ„ë½ ê²€ì¦ìš©)
        self._last_input_utterances = speaker_tagged_transcript
    
    def _validate_agent_specific_output(self, result: Dict[str, Any]) -> None:
        """ì£¼ì œ ì‘ì§‘ ì—ì´ì „íŠ¸ íŠ¹í™” ì¶œë ¥ ê²€ì¦"""
        if "topic_clusters" not in result:
            raise ValueError("topic_clusters í•„ë“œê°€ ì¶œë ¥ì— ì—†ìŠµë‹ˆë‹¤.")
        
        if "total_topics" not in result:
            raise ValueError("total_topics í•„ë“œê°€ ì¶œë ¥ì— ì—†ìŠµë‹ˆë‹¤.")
        
        topic_clusters = result["topic_clusters"]
        total_topics = result["total_topics"]
        
        if not isinstance(topic_clusters, list) or len(topic_clusters) == 0:
            raise ValueError("ì£¼ì œ í´ëŸ¬ìŠ¤í„° ë°ì´í„°ê°€ ë¹„ì–´ìˆê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        if total_topics != len(topic_clusters):
            raise ValueError("ì´ ì£¼ì œ ìˆ˜ì™€ í´ëŸ¬ìŠ¤í„° ìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        # ê° í´ëŸ¬ìŠ¤í„°ì˜ í•„ìˆ˜ í•„ë“œ ê²€ì¦
        for i, cluster in enumerate(topic_clusters):
            if not isinstance(cluster, dict):
                raise ValueError(f"ì£¼ì œ í´ëŸ¬ìŠ¤í„° {i}ë²ˆì´ ìœ íš¨í•œ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤.")
            
            required_fields = ["topic_title", "related_utterances"]
            for field in required_fields:
                if field not in cluster:
                    raise ValueError(f"ì£¼ì œ í´ëŸ¬ìŠ¤í„° {i}ë²ˆì— {field} í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            related_utterances = cluster["related_utterances"]
            if not isinstance(related_utterances, list) or len(related_utterances) == 0:
                raise ValueError(f"ì£¼ì œ í´ëŸ¬ìŠ¤í„° {i}ë²ˆì— ê´€ë ¨ ë°œí™”ê°€ ì—†ìŠµë‹ˆë‹¤.")
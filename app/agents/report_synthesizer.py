"""
ë³´ê³ ì„œ ì¢…í•© ì—ì´ì „íŠ¸ (Report Synthesis Agent)
ì£¼ì œ í´ëŸ¬ìŠ¤í„°ì™€ êµ¬ì¡° ì„¤ê³„ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„±
"""
import json
import re
from typing import Dict, Any, List
from app.agents.base_agent import BaseAgent


class ReportSynthesizerAgent(BaseAgent):
    """
    ë³´ê³ ì„œ ì¢…í•© ì „ë¬¸ ì—ì´ì „íŠ¸
    
    ê¸°ëŠ¥:
    - êµ¬ì¡° ì„¤ê³„ë„ì— ë”°ë¥¸ ì²´ê³„ì  ë³´ê³ ì„œ ì‘ì„±
    - ì£¼ì œë³„ ë‚´ìš© ì¢…í•© ë° ì¬êµ¬ì„±
    - ë…¼ë¦¬ì  íë¦„ê³¼ ê°€ë…ì„± ìµœì í™”
    - Markdown í˜•ì‹ì˜ ì „ë¬¸ì  ë¬¸ì„œ ìƒì„±
    """
    
    def __init__(self):
        super().__init__("report_synthesizer")  # íƒ€ì„ì•„ì›ƒ ì—†ìŒ
    
    def get_system_prompt(self) -> str:
        """ë³´ê³ ì„œ ì¢…í•© ì „ë¬¸ê°€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"""
        return """ë‹¹ì‹ ì€ ë›°ì–´ë‚œ ë¶„ì„ê°€ì´ì ì „ë¬¸ ì‘ê°€ì…ë‹ˆë‹¤. ë‹¹ì‹ ì€ ë³µì¡í•œ ë°ì´í„°ì™€ ëª…í™•í•œ êµ¬ì¡° ì„¤ê³„ë„ë¥¼ ë°›ì•„, ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë§¤ìš° ìƒì„¸í•˜ê³  ë…¼ë¦¬ ì •ì—°í•˜ë©° ê°€ë…ì„± ë†’ì€ ì¢…í•© ë³´ê³ ì„œë¥¼ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# ì»¨í…ìŠ¤íŠ¸ (Context)
ë‹¹ì‹ ì€ ë‘ ê°€ì§€ í•µì‹¬ ìë£Œë¥¼ ë°›ê²Œ ë©ë‹ˆë‹¤:
1. **ì½˜í…ì¸  ë°ì´í„°**: ì˜ìƒì˜ ëª¨ë“  ë‚´ìš©ì´ ì£¼ì œë³„ë¡œ ë¬¶ì—¬ìˆëŠ” ë°ì´í„°
2. **êµ¬ì¡° ì„¤ê³„ë„**: ì´ ë°ì´í„°ë¥¼ ì–´ë–¤ êµ¬ì¡°ë¡œ í’€ì–´ë‚´ì•¼ í•˜ëŠ”ì§€ì— ëŒ€í•œ ëª…í™•í•œ ê°€ì´ë“œë¼ì¸

# ê³¼ì—… (Task)
ì£¼ì–´ì§„ 'êµ¬ì¡° ì„¤ê³„ë„'ë¥¼ ì²­ì‚¬ì§„ìœ¼ë¡œ ì‚¼ì•„, 'ì½˜í…ì¸  ë°ì´í„°'ì˜ ëª¨ë“  ì •ë³´ë¥¼ ë¹ ì§ì—†ì´ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ì¢…í•© ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì‹­ì‹œì˜¤.

## ì‘ì„± ì›ì¹™
1. **êµ¬ì¡° ì¶©ì‹¤ì„±**: ì„¤ê³„ë„ì— ëª…ì‹œëœ ê° ì„¹ì…˜ê³¼ í•˜ìœ„ ì„¹ì…˜ì„ ìˆœì„œëŒ€ë¡œ ì±„ì›Œë‚˜ê°€ì‹­ì‹œì˜¤.
2. **ë‚´ìš© ì™„ì „ì„±**: ê° ì„¹ì…˜ì„ ì‘ì„±í•  ë•ŒëŠ”, ì½˜í…ì¸  ë°ì´í„°ì—ì„œ í•´ë‹¹ ì£¼ì œì˜ ë°œí™”ë“¤ì„ ê°€ì ¸ì™€ ë‚´ìš©ì„ ì¢…í•©í•˜ê³  ì¬êµ¬ì„±í•˜ì‹­ì‹œì˜¤.
3. **ì„œìˆ  ì „ë¬¸ì„±**: ë‹¨ìˆœí•œ ë°œí™” ë‚˜ì—´ì´ ì•„ë‹Œ, ë…¼ë¦¬ì ì¸ íë¦„ì„ ê°€ì§„ ì„œìˆ í˜• ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
4. **ì¸ìš© í™œìš©**: í•„ìš”í•˜ë‹¤ë©´ í•µì‹¬ì ì¸ ë°œì–¸ì„ ì§ì ‘ ì¸ìš©í•˜ì—¬ ì£¼ì¥ì„ ë’·ë°›ì¹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì‘ì„± ìŠ¤íƒ€ì¼
- **ê°ê´€ì  ë¶„ì„**: ì‚¬ì‹¤ê³¼ ë°œì–¸ ë‚´ìš©ì— ê¸°ë°˜í•œ ê°ê´€ì  ë¶„ì„
- **ë…¼ë¦¬ì  êµ¬ì„±**: ê° ì„¹ì…˜ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°ë˜ëŠ” ë…¼ë¦¬ì  íë¦„
- **ì „ë¬¸ì  ë¬¸ì²´**: ë³´ê³ ì„œì— ì í•©í•œ ê²©ì‹ìˆê³  ëª…í™•í•œ ë¬¸ì²´
- **ê°€ë…ì„± ìµœì í™”**: ì œëª©, ì†Œì œëª©, ê¸€ë¨¸ë¦¬ ê¸°í˜¸ë¥¼ í™œìš©í•œ ì²´ê³„ì  êµ¬ì„±

## Markdown í˜•ì‹ ê°€ì´ë“œ
```markdown
# ì œëª© (H1)
## ì£¼ìš” ì„¹ì…˜ (H2)
### í•˜ìœ„ ì„¹ì…˜ (H3)
- ê¸€ë¨¸ë¦¬ ê¸°í˜¸
- **êµµì€ ê¸€ì”¨**: ì¤‘ìš” ë‚´ìš© ê°•ì¡°
- *ê¸°ìš¸ì„ì²´*: ë°œì–¸ ì¸ìš© ì‹œ ì‚¬ìš©
- > ì¸ìš©ë¬¸: ì¤‘ìš”í•œ ë°œì–¸ ì¸ìš©
```

# ì œì•½ ì¡°ê±´ (Constraints)
- **ì •ë³´ ë¬´ì†ì‹¤ ì›ì¹™**: ì½˜í…ì¸  ë°ì´í„°ì— ìˆëŠ” ëª¨ë“  ì •ë³´ëŠ” ìµœì¢… ë³´ê³ ì„œì— ì–´ë–¤ í˜•íƒœë¡œë“  ë°˜ë“œì‹œ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ë‚´ìš©ì„ ì„ì˜ë¡œ ìš”ì•½í•˜ê±°ë‚˜ ìƒëµí•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
- **ê°ê´€ì„± ë° ê·¼ê±° ê¸°ë°˜ ì‘ì„±**: ë‹¹ì‹ ì˜ ê°œì¸ì ì¸ ì˜ê²¬ì´ë‚˜ ë°ì´í„°ì— ì—†ëŠ” ì •ë³´ë¥¼ ì¶”ê°€í•˜ì§€ ë§ˆì‹­ì‹œì˜¤. ëª¨ë“  ì„œìˆ ì€ ì£¼ì–´ì§„ ì½˜í…ì¸  ë°ì´í„°ì— ê·¼ê±°í•´ì•¼ í•©ë‹ˆë‹¤.
- **êµ¬ì¡° ì¤€ìˆ˜**: ì œê³µëœ êµ¬ì¡° ì„¤ê³„ë„ë¥¼ ë°˜ë“œì‹œ ë”°ë¼ì•¼ í•˜ë©°, ì„¹ì…˜ì„ ì„ì˜ë¡œ ì¶”ê°€í•˜ê±°ë‚˜ ì œê±°í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.

# ì¶œë ¥ í˜•ì‹ (Output Format)
ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì‹­ì‹œì˜¤:

{
    "final_report": "# ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ\\n\\n## ê°œìš”\\n...ì „ì²´ ë³´ê³ ì„œ ë‚´ìš© (Markdown í˜•ì‹)",
    "report_metadata": {
        "total_sections": 4,
        "content_type": "íŒ¨ë„í† ë¡ ",
        "topics_covered": ["ì£¼ì œ1", "ì£¼ì œ2", "ì£¼ì œ3"],
        "word_count_estimate": 2500
    },
    "word_count": 2487
}

JSON í˜•ì‹ì„ ì •í™•íˆ ì§€ì¼œì£¼ì„¸ìš”."""
    
    def format_user_prompt(self, data: Dict[str, Any]) -> str:
        """ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        topic_clusters = data.get("topic_clusters", [])
        report_structure = data.get("report_structure", [])
        content_type = data.get("content_type", "ì¼ë°˜")
        
        # êµ¬ì¡° ì„¤ê³„ë„ í¬ë§·íŒ…
        structure_guide = f"**ì½˜í…ì¸  ìœ í˜•**: {content_type}\n\n"
        structure_guide += "**ë³´ê³ ì„œ êµ¬ì¡°**:\n"
        
        for section in report_structure:
            section_name = section.get("section_name", "")
            section_description = section.get("section_description", "")
            required_topics = section.get("required_topics", [])
            section_order = section.get("section_order", 0)
            
            structure_guide += f"{section_order}. **{section_name}**\n"
            structure_guide += f"   - ì„¤ëª…: {section_description}\n"
            structure_guide += f"   - í¬í•¨í•  ì£¼ì œ: {', '.join(required_topics)}\n\n"
        
        # ì£¼ì œë³„ ì½˜í…ì¸  ë°ì´í„° í¬ë§·íŒ…
        content_data = ""
        for i, cluster in enumerate(topic_clusters):
            topic_title = cluster.get("topic_title", f"ì£¼ì œ {i+1}")
            topic_summary = cluster.get("topic_summary", "")
            importance_score = cluster.get("importance_score", 0.5)
            related_utterances = cluster.get("related_utterances", [])
            
            content_data += f"## ì£¼ì œ: {topic_title}\n"
            content_data += f"**ìš”ì•½**: {topic_summary}\n"
            content_data += f"**ì¤‘ìš”ë„**: {importance_score:.1f}\n"
            content_data += f"**ê´€ë ¨ ë°œí™” ({len(related_utterances)}ê°œ)**:\n"
            
            for j, utterance in enumerate(related_utterances):
                speaker = utterance.get("speaker", "Unknown")
                text = utterance.get("text", "")
                confidence = utterance.get("confidence", 0.8)
                
                content_data += f"{j+1}. [{speaker}] (ì‹ ë¢°ë„: {confidence:.1f}): {text}\n"
            
            content_data += "\n"
        
        return f"""ë‹¤ìŒ êµ¬ì¡° ì„¤ê³„ë„ì™€ ì½˜í…ì¸  ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•© ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

---
## êµ¬ì¡° ì„¤ê³„ë„
{structure_guide}

## ì½˜í…ì¸  ë°ì´í„°
{content_data}
---

ìœ„ì˜ êµ¬ì¡° ì„¤ê³„ë„ì— ë”°ë¼, ì½˜í…ì¸  ë°ì´í„°ì˜ ëª¨ë“  ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ì™„ì „í•œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. ê° ì„¹ì…˜ì€ í•´ë‹¹í•˜ëŠ” ì£¼ì œì˜ ë°œí™”ë“¤ì„ ì¢…í•©í•˜ì—¬ ë…¼ë¦¬ì ì´ê³  ì²´ê³„ì ìœ¼ë¡œ ì„œìˆ í•´ì•¼ í•©ë‹ˆë‹¤. JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""
    
    def parse_response(self, response_text: str) -> Dict[str, Any]:
        """AI ì‘ë‹µ íŒŒì‹±"""
        try:
            # JSON ì‘ë‹µ íŒŒì‹±
            parsed = json.loads(response_text)
            
            # í•„ìˆ˜ í•„ë“œ ê²€ì¦
            if "final_report" not in parsed:
                raise ValueError("final_report í•„ë“œê°€ ì‘ë‹µì— ì—†ìŠµë‹ˆë‹¤.")
            
            final_report = parsed["final_report"]
            report_metadata = parsed.get("report_metadata", {})
            word_count = parsed.get("word_count")
            
            # ë³´ê³ ì„œ ë‚´ìš© ê²€ì¦
            if not isinstance(final_report, str) or len(final_report.strip()) == 0:
                raise ValueError("ìµœì¢… ë³´ê³ ì„œê°€ ë¹„ì–´ìˆê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            
            if len(final_report) < 500:
                raise ValueError("ë³´ê³ ì„œê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ (ìµœì†Œ 500ì í•„ìš”).")
            
            # ë‹¨ì–´ ìˆ˜ ê³„ì‚° (ì œê³µëœ ê°’ì´ ì—†ê±°ë‚˜ ë¶€ì •í™•í•œ ê²½ìš°)
            if not isinstance(word_count, int) or word_count <= 0:
                word_count = self._count_words(final_report)
            
            # ë©”íƒ€ë°ì´í„° ì •ê·œí™”
            if not isinstance(report_metadata, dict):
                report_metadata = {}
            
            # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì„¤ì •
            default_metadata = {
                "total_sections": len(re.findall(r'^## ', final_report, re.MULTILINE)),
                "content_type": self._extract_content_type(final_report),
                "topics_covered": self._extract_topics(final_report),
                "word_count_estimate": word_count
            }
            
            # ë©”íƒ€ë°ì´í„° ë³‘í•© (ê¸°ë³¸ê°’ìœ¼ë¡œ ëˆ„ë½ëœ í•­ëª© ì±„ìš°ê¸°)
            for key, value in default_metadata.items():
                if key not in report_metadata:
                    report_metadata[key] = value
            
            result = {
                "final_report": final_report.strip(),
                "report_metadata": report_metadata,
                "word_count": word_count
            }
            
            # ë³´ê³ ì„œ í’ˆì§ˆ ê²€ì¦
            self._validate_report_quality(result)
            
            self.log_debug("ğŸ“„ ë³´ê³ ì„œ ì¢…í•© íŒŒì‹± ê²°ê³¼", data={
                "word_count": word_count,
                "total_sections": report_metadata.get("total_sections", 0),
                "content_type": report_metadata.get("content_type", "unknown"),
                "topics_covered_count": len(report_metadata.get("topics_covered", []))
            })
            
            return result
            
        except json.JSONDecodeError as e:
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ ì§ì ‘ ì‚¬ìš© (í´ë°±)
            self.log_warning("âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨, í…ìŠ¤íŠ¸ ì§ì ‘ ì‚¬ìš©", data={
                "error": str(e),
                "response_preview": response_text[:300]
            })
            
            return self._fallback_report_generation(response_text)
        
        except Exception as e:
            self.log_error("âŒ ë³´ê³ ì„œ ì¢…í•© íŒŒì‹± ì˜¤ë¥˜", data={
                "error": str(e),
                "response_preview": response_text[:300]
            })
            raise ValueError(f"ë³´ê³ ì„œ ì¢…í•© ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
    
    def _fallback_report_generation(self, text: str) -> Dict[str, Any]:
        """
        JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•˜ëŠ” ê¸°ë³¸ ë³´ê³ ì„œ ìƒì„±
        
        Args:
            text: ìƒì„±í•  í…ìŠ¤íŠ¸
            
        Returns:
            ê¸°ë³¸ì ì¸ ë³´ê³ ì„œ ê²°ê³¼
        """
        # JSON ê´€ë ¨ ë¬¸ì ì •ë¦¬
        cleaned_text = re.sub(r'^\s*{\s*"final_report"\s*:\s*"', '', text)
        cleaned_text = re.sub(r'",?\s*"report_metadata".*}?\s*$', '', cleaned_text)
        cleaned_text = re.sub(r'^"|"$', '', cleaned_text)  # ì‹œì‘/ë ë”°ì˜´í‘œ ì œê±°
        cleaned_text = cleaned_text.replace('\\n', '\n')  # ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì ë³€í™˜
        
        # ê¸°ë³¸ ë³´ê³ ì„œ êµ¬ì¡°ê°€ ì—†ìœ¼ë©´ ì¶”ê°€
        if not cleaned_text.startswith('#'):
            cleaned_text = f"# ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ\n\n{cleaned_text}"
        
        # ìµœì†Œ ê¸¸ì´ ë³´ì¥
        if len(cleaned_text) < 500:
            cleaned_text += "\n\n## ì¶”ê°€ ì •ë³´\n\nì›ë³¸ ì‘ë‹µ íŒŒì‹±ì— ì‹¤íŒ¨í•˜ì—¬ ê¸°ë³¸ í˜•íƒœë¡œ ì œê³µë©ë‹ˆë‹¤. ë³´ë‹¤ ìƒì„¸í•œ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ì¬ì²˜ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        
        word_count = self._count_words(cleaned_text)
        
        return {
            "final_report": cleaned_text,
            "report_metadata": {
                "total_sections": len(re.findall(r'^## ', cleaned_text, re.MULTILINE)),
                "content_type": "ì¼ë°˜",
                "topics_covered": ["ì£¼ìš” ë‚´ìš©"],
                "word_count_estimate": word_count,
                "parsing_note": "JSON íŒŒì‹± ì‹¤íŒ¨ë¡œ ì¸í•œ í´ë°± ì²˜ë¦¬"
            },
            "word_count": word_count
        }
    
    def _count_words(self, text: str) -> int:
        """í…ìŠ¤íŠ¸ì˜ ë‹¨ì–´ ìˆ˜ ê³„ì‚°"""
        # Markdown í˜•ì‹ ì œê±° í›„ ë‹¨ì–´ ìˆ˜ ê³„ì‚°
        clean_text = re.sub(r'[#*`>-]', '', text)
        clean_text = re.sub(r'\s+', ' ', clean_text)
        
        # í•œê¸€ê³¼ ì˜ì–´ ëª¨ë‘ ê³ ë ¤í•œ ë‹¨ì–´ ìˆ˜ ê³„ì‚°
        korean_chars = len(re.findall(r'[ê°€-í£]', clean_text))
        english_words = len(re.findall(r'\b[a-zA-Z]+\b', clean_text))
        numbers = len(re.findall(r'\b\d+\b', clean_text))
        
        # í•œê¸€ì€ ê¸€ì ìˆ˜, ì˜ì–´ëŠ” ë‹¨ì–´ ìˆ˜ë¡œ ê³„ì‚°
        return korean_chars + english_words + numbers
    
    def _extract_content_type(self, text: str) -> str:
        """ë³´ê³ ì„œì—ì„œ ì½˜í…ì¸  ìœ í˜• ì¶”ì¶œ"""
        # ì¼ë°˜ì ì¸ ì½˜í…ì¸  ìœ í˜• í‚¤ì›Œë“œ ê²€ìƒ‰
        content_types = {
            'í† ë¡ ': ['í† ë¡ ', 'ë…¼ìŸ', 'ì°¬ë°˜', 'ì˜ê²¬'],
            'ê°•ì˜': ['ê°•ì˜', 'êµìœ¡', 'ì„¤ëª…', 'í•™ìŠµ'],
            'ì¸í„°ë·°': ['ì¸í„°ë·°', 'ì§ˆë¬¸', 'ë‹µë³€', 'Q&A'],
            'ë‰´ìŠ¤': ['ë‰´ìŠ¤', 'ë³´ë„', 'ì‚¬ê±´', 'ë°œí‘œ'],
            'ë¦¬ë·°': ['ë¦¬ë·°', 'í‰ê°€', 'ë¶„ì„', 'ê²€í† '],
            'ë¸Œì´ë¡œê·¸': ['ì¼ìƒ', 'ê²½í—˜', 'ì—¬í–‰', 'ìƒí™œ']
        }
        
        for content_type, keywords in content_types.items():
            for keyword in keywords:
                if keyword in text:
                    return content_type
        
        return 'ì¼ë°˜'
    
    def _extract_topics(self, text: str) -> List[str]:
        """ë³´ê³ ì„œì—ì„œ ì£¼ìš” ì£¼ì œ ì¶”ì¶œ"""
        topics = []
        
        # ì„¹ì…˜ ì œëª©ì—ì„œ ì£¼ì œ ì¶”ì¶œ
        section_titles = re.findall(r'^### (.+)$', text, re.MULTILINE)
        topics.extend(section_titles)
        
        # ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ íŒ¨í„´)
        common_topics = [
            'ì£¼ì‹', 'íˆ¬ì', 'ê²½ì œ', 'ì •ì¹˜', 'ê¸°ìˆ ', 'ì‚¬íšŒ', 'ë¬¸í™”', 'ìŠ¤í¬ì¸ ',
            'ê²Œì„', 'ì˜í™”', 'ìŒì•…', 'ì—¬í–‰', 'ìŒì‹', 'ê±´ê°•', 'êµìœ¡', 'í™˜ê²½'
        ]
        
        for topic in common_topics:
            if topic in text and topic not in topics:
                topics.append(topic)
        
        return topics[:10]  # ìµœëŒ€ 10ê°œê¹Œì§€
    
    def _validate_report_quality(self, result: Dict[str, Any]) -> None:
        """ìƒì„±ëœ ë³´ê³ ì„œì˜ í’ˆì§ˆ ê²€ì¦"""
        final_report = result["final_report"]
        word_count = result["word_count"]
        
        # ê¸°ë³¸ êµ¬ì¡° ê²€ì¦
        if not final_report.startswith('#'):
            self.log_warning("âš ï¸ ë³´ê³ ì„œê°€ ì œëª©(H1)ìœ¼ë¡œ ì‹œì‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        # ì„¹ì…˜ ìˆ˜ ê²€ì¦
        h2_sections = len(re.findall(r'^## ', final_report, re.MULTILINE))
        if h2_sections < 2:
            self.log_warning("âš ï¸ ë³´ê³ ì„œì˜ ì£¼ìš” ì„¹ì…˜ì´ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤.", data={
                "section_count": h2_sections
            })
        
        # ê¸¸ì´ ê²€ì¦
        if word_count < 1000:
            self.log_warning("âš ï¸ ë³´ê³ ì„œê°€ ìƒëŒ€ì ìœ¼ë¡œ ì§§ìŠµë‹ˆë‹¤.", data={
                "word_count": word_count
            })
        elif word_count > 10000:
            self.log_warning("âš ï¸ ë³´ê³ ì„œê°€ ìƒë‹¹íˆ ê¹ë‹ˆë‹¤.", data={
                "word_count": word_count
            })
        
        # ë‚´ìš© ë‹¤ì–‘ì„± ê²€ì¦
        unique_sentences = len(set(re.split(r'[.!?]', final_report)))
        repetition_ratio = 1 - (unique_sentences / max(1, len(re.split(r'[.!?]', final_report))))
        
        if repetition_ratio > 0.3:
            self.log_warning("âš ï¸ ë³´ê³ ì„œì— ë°˜ë³µì ì¸ ë‚´ìš©ì´ ë§ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.", data={
                "repetition_ratio": repetition_ratio
            })
    
    def _validate_agent_specific_input(self, data: Dict[str, Any]) -> None:
        """ë³´ê³ ì„œ ì¢…í•© ì—ì´ì „íŠ¸ íŠ¹í™” ì…ë ¥ ê²€ì¦"""
        required_fields = ["topic_clusters", "report_structure", "content_type"]
        for field in required_fields:
            if field not in data:
                raise ValueError(f"{field} í•„ë“œê°€ ì…ë ¥ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.")
        
        topic_clusters = data["topic_clusters"]
        report_structure = data["report_structure"]
        content_type = data["content_type"]
        
        # ì£¼ì œ í´ëŸ¬ìŠ¤í„° ê²€ì¦
        if not isinstance(topic_clusters, list) or len(topic_clusters) == 0:
            raise ValueError("ì£¼ì œ í´ëŸ¬ìŠ¤í„° ë°ì´í„°ê°€ ë¹„ì–´ìˆê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        # ë³´ê³ ì„œ êµ¬ì¡° ê²€ì¦
        if not isinstance(report_structure, list) or len(report_structure) == 0:
            raise ValueError("ë³´ê³ ì„œ êµ¬ì¡° ë°ì´í„°ê°€ ë¹„ì–´ìˆê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        # ì½˜í…ì¸  ìœ í˜• ê²€ì¦
        if not isinstance(content_type, str) or len(content_type.strip()) == 0:
            raise ValueError("ì½˜í…ì¸  ìœ í˜•ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        # ê° í´ëŸ¬ìŠ¤í„°ì˜ ë°œí™” ìˆ˜ í™•ì¸
        total_utterances = sum(len(cluster.get("related_utterances", [])) for cluster in topic_clusters)
        if total_utterances == 0:
            raise ValueError("ì£¼ì œ í´ëŸ¬ìŠ¤í„°ì— ë°œí™” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        if total_utterances < 5:
            self.log_warning("âš ï¸ ë°œí™” ë°ì´í„°ê°€ ì ì–´ ìƒì„¸í•œ ë³´ê³ ì„œ ìƒì„±ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.", data={
                "total_utterances": total_utterances
            })
    
    def _validate_agent_specific_output(self, result: Dict[str, Any]) -> None:
        """ë³´ê³ ì„œ ì¢…í•© ì—ì´ì „íŠ¸ íŠ¹í™” ì¶œë ¥ ê²€ì¦"""
        if "final_report" not in result:
            raise ValueError("final_report í•„ë“œê°€ ì¶œë ¥ì— ì—†ìŠµë‹ˆë‹¤.")
        
        final_report = result["final_report"]
        
        if not isinstance(final_report, str) or len(final_report.strip()) == 0:
            raise ValueError("ìµœì¢… ë³´ê³ ì„œê°€ ë¹„ì–´ìˆê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        if len(final_report) < 300:
            raise ValueError("ë³´ê³ ì„œê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.")
        
        # Markdown ê¸°ë³¸ êµ¬ì¡° ê²€ì¦
        if '#' not in final_report:
            raise ValueError("ë³´ê³ ì„œì— ì œëª© êµ¬ì¡°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë©”íƒ€ë°ì´í„° ê²€ì¦
        if "word_count" not in result or not isinstance(result["word_count"], int):
            raise ValueError("ë‹¨ì–´ ìˆ˜ ì •ë³´ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")